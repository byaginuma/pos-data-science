{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f98ba25-8f47-48ed-aa35-172595a07424",
   "metadata": {},
   "source": [
    "## 0. Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904bf8a-4d96-4b52-98c6-ac1e7f40c582",
   "metadata": {},
   "source": [
    "## 1. Importando dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd53e22b-cc2f-4135-a8b2-a92a88256683",
   "metadata": {},
   "source": [
    "### 1.1. Importando dados do S3 para pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9453e18f-3f88-449c-a2b6-fba31085317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# criando sessão do s3\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id = 'AKIAQ7W6N2BJJAHR5AKF',\n",
    "    aws_secret_access_key='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'\n",
    ")\n",
    "\n",
    "#criando objeto s3, que vai criar a conexão\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "#criando o arquivo que vamos importar do s3\n",
    "s3object = s3.Object('beatriz-yaginuma','trabalho-computacao-nuvem/US_Accidents_Dec20_updated.csv')\n",
    "s3object.get()\n",
    "\n",
    "#fazendo chamada e jogando dados num objeto\n",
    "file = s3object.get()['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c18590-046e-4eb3-9acd-f137ba7d14b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.2 s, sys: 1.16 s, total: 8.36 s\n",
      "Wall time: 8.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "#importando no pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcac6364-9805-4d6f-bd83-4b8b26e10495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1516064, 47)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253993f-07bd-42d6-af03-9299fed0f529",
   "metadata": {},
   "source": [
    "### 1.2. Importando do Pandas para Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d031d6b-1837-4dd5-a6f3-78f4a97327b7",
   "metadata": {},
   "source": [
    "Ao invés de criar um contexto para cada interação específica do Spark, vamos criar uma sessão, que é uma combinação de todos os contextos diferentes do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94c2755-462b-46b3-9193-852721be34ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 18:53:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('trabalho') \\\n",
    "                    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a49a4ef-d1d9-42a5-a9df-e2800df1ec08",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "field Weather_Condition: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16/613420364.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#criando dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_pandas\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;31m# Create a DataFrame from pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             return super(SparkSession, self).createDataFrame(\n\u001b[0m\u001b[1;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can not infer schema from empty dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0mnfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[0m\u001b[1;32m   1108\u001b[0m                                                   name=new_name(f.name)))\n\u001b[1;32m   1109\u001b[0m                   for f in a.fields]\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0mnfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[0m\u001b[1;32m   1108\u001b[0m                                                   name=new_name(f.name)))\n\u001b[1;32m   1109\u001b[0m                   for f in a.fields]\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;31m# TODO: type cast (such as int -> long)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not merge type %s and %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;31m# same type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: field Weather_Condition: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
     ]
    }
   ],
   "source": [
    "#criando dataframe\n",
    "spark_df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f534e-c243-4f2d-8069-2f7c814ae3c2",
   "metadata": {},
   "source": [
    "Problema: algumas colunas na base tem valores ausentes, o que leva o Pandas a representá-los como tipos mistos (string para não ausentes, NaN para valores ausentes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352094b-3c7a-4f2b-9752-f4ebbc6e2cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(pandas_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388f0c9-35d4-4034-b304-c56ed9277709",
   "metadata": {},
   "source": [
    "Para resolver isso, precisamos fornecer um schema para a função createDataFrame.\n",
    "Vamos usar algumas funções para automatizar a construção do schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e6543a-640b-4adf-a938-15ea5ac794ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Auxiliar functions\n",
    "def equivalent_type(f):\n",
    "    if f == 'datetime64[ns]': return TimestampType()\n",
    "    elif f == 'int64': return LongType()\n",
    "    elif f == 'int32': return IntegerType()\n",
    "    elif f == 'float64': return FloatType()\n",
    "    elif f == 'bool': return BooleanType()\n",
    "    else: return StringType()\n",
    "\n",
    "def define_structure(string, pandas_dtype):\n",
    "    try: spark_dtype = equivalent_type(pandas_dtype)\n",
    "    except: spark_dtype = StringType()\n",
    "    return StructField(string, spark_dtype)\n",
    "\n",
    "# Given pandas dataframe, it will return a spark's dataframe.\n",
    "def define_schema(pandas_df):\n",
    "    columns = list(pandas_df.columns)\n",
    "    types = list(pandas_df.dtypes)\n",
    "    struct_list = []\n",
    "    for column, typo in zip(columns, types): \n",
    "      struct_list.append(define_structure(column, typo))\n",
    "    return StructType(struct_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984853aa-edf7-4f1d-aba9-bf30e70eeab8",
   "metadata": {},
   "source": [
    "- A primeira função tem como input o tipo de dados do pandas, e retorna o tipo equivalente do Spark.\n",
    "- A segunda função tem como input o nome de uma variável e seu tipo de dados do pandas, e aplica a primeira função para retornar o StructField da variável + o tipo de dados do Spark\n",
    "- A terceira função tem como input o dataframe do pandas, e constro listas de todas as columas e seus respectivos tipos do dataframe para aplicar a segunda função e criar uma terceira lista de todos os struct types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66407dfc-7c90-490e-9722-bed3fcb923d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(ID,StringType,true),StructField(Severity,LongType,true),StructField(Start_Time,StringType,true),StructField(End_Time,StringType,true),StructField(Start_Lat,FloatType,true),StructField(Start_Lng,FloatType,true),StructField(End_Lat,FloatType,true),StructField(End_Lng,FloatType,true),StructField(Distance(mi),FloatType,true),StructField(Description,StringType,true),StructField(Number,FloatType,true),StructField(Street,StringType,true),StructField(Side,StringType,true),StructField(City,StringType,true),StructField(County,StringType,true),StructField(State,StringType,true),StructField(Zipcode,StringType,true),StructField(Country,StringType,true),StructField(Timezone,StringType,true),StructField(Airport_Code,StringType,true),StructField(Weather_Timestamp,StringType,true),StructField(Temperature(F),FloatType,true),StructField(Wind_Chill(F),FloatType,true),StructField(Humidity(%),FloatType,true),StructField(Pressure(in),FloatType,true),StructField(Visibility(mi),FloatType,true),StructField(Wind_Direction,StringType,true),StructField(Wind_Speed(mph),FloatType,true),StructField(Precipitation(in),FloatType,true),StructField(Weather_Condition,StringType,true),StructField(Amenity,BooleanType,true),StructField(Bump,BooleanType,true),StructField(Crossing,BooleanType,true),StructField(Give_Way,BooleanType,true),StructField(Junction,BooleanType,true),StructField(No_Exit,BooleanType,true),StructField(Railway,BooleanType,true),StructField(Roundabout,BooleanType,true),StructField(Station,BooleanType,true),StructField(Stop,BooleanType,true),StructField(Traffic_Calming,BooleanType,true),StructField(Traffic_Signal,BooleanType,true),StructField(Turning_Loop,BooleanType,true),StructField(Sunrise_Sunset,StringType,true),StructField(Civil_Twilight,StringType,true),StructField(Nautical_Twilight,StringType,true),StructField(Astronomical_Twilight,StringType,true)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a função\n",
    "p_schema = define_schema(pandas_df)\n",
    "p_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d98d74b6-8115-4772-9fd6-bcc18784a480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.1 s, sys: 795 ms, total: 50.8 s\n",
      "Wall time: 52.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Importando os dados para o Spark, agora definindo o Schema\n",
    "spark_df = spark.createDataFrame(pandas_df, p_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3e314-c90c-47aa-bc9e-a4d2c757b04d",
   "metadata": {},
   "source": [
    "## 2. Limpeza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b2d23-2b52-4f23-b454-ca191fc8b3ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1. Descrição da base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a55facb9-01aa-431b-a515-e513d5abff33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Severity: long (nullable = true)\n",
      " |-- Start_Time: string (nullable = true)\n",
      " |-- End_Time: string (nullable = true)\n",
      " |-- Start_Lat: float (nullable = true)\n",
      " |-- Start_Lng: float (nullable = true)\n",
      " |-- End_Lat: float (nullable = true)\n",
      " |-- End_Lng: float (nullable = true)\n",
      " |-- Distance(mi): float (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Number: float (nullable = true)\n",
      " |-- Street: string (nullable = true)\n",
      " |-- Side: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Timezone: string (nullable = true)\n",
      " |-- Airport_Code: string (nullable = true)\n",
      " |-- Weather_Timestamp: string (nullable = true)\n",
      " |-- Temperature(F): float (nullable = true)\n",
      " |-- Wind_Chill(F): float (nullable = true)\n",
      " |-- Humidity(%): float (nullable = true)\n",
      " |-- Pressure(in): float (nullable = true)\n",
      " |-- Visibility(mi): float (nullable = true)\n",
      " |-- Wind_Direction: string (nullable = true)\n",
      " |-- Wind_Speed(mph): float (nullable = true)\n",
      " |-- Precipitation(in): float (nullable = true)\n",
      " |-- Weather_Condition: string (nullable = true)\n",
      " |-- Amenity: boolean (nullable = true)\n",
      " |-- Bump: boolean (nullable = true)\n",
      " |-- Crossing: boolean (nullable = true)\n",
      " |-- Give_Way: boolean (nullable = true)\n",
      " |-- Junction: boolean (nullable = true)\n",
      " |-- No_Exit: boolean (nullable = true)\n",
      " |-- Railway: boolean (nullable = true)\n",
      " |-- Roundabout: boolean (nullable = true)\n",
      " |-- Station: boolean (nullable = true)\n",
      " |-- Stop: boolean (nullable = true)\n",
      " |-- Traffic_Calming: boolean (nullable = true)\n",
      " |-- Traffic_Signal: boolean (nullable = true)\n",
      " |-- Turning_Loop: boolean (nullable = true)\n",
      " |-- Sunrise_Sunset: string (nullable = true)\n",
      " |-- Civil_Twilight: string (nullable = true)\n",
      " |-- Nautical_Twilight: string (nullable = true)\n",
      " |-- Astronomical_Twilight: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analisando os dados\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c292683d-ddfb-49b2-8646-5141734f5645",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2.1.1 Descrição das variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470bd6b0-cdc1-4ee1-be08-65b75f78f449",
   "metadata": {},
   "source": [
    "Traffic Attributes (10):\n",
    "\n",
    "- ID: This is a unique identifier of the accident record.\n",
    "- Severity: Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on traffic (i.e., short delay as a result of the accident) and 4 indicates a significant impact on traffic (i.e., long delay).\n",
    "- Start_Time: Shows start time of the accident in local time zone.\n",
    "- End_Time: Shows end time of the accident in local time zone.\n",
    "- Start_Lat: Shows latitude in GPS coordinate of the start point.\n",
    "- Start_Lng: Shows longitude in GPS coordinate of the start point.\n",
    "- End_Lat: Shows latitude in GPS coordinate of the end point.\n",
    "- End_Lng: Shows longitude in GPS coordinate of the end point.\n",
    "- Distance(mi): The length of the road extent affected by the accident.\n",
    "- Description: Shows natural language description of the accident.\n",
    "\n",
    "Address Attributes (9):\n",
    "\n",
    "- Number: Shows the street number in address field.\n",
    "- Street: Shows the street name in address field.\n",
    "- Side: Shows the relative side of the street (Right/Left) in address field.\n",
    "- City: Shows the city in address field.\n",
    "- County: Shows the county in address field.\n",
    "- State: Shows the state in address field.\n",
    "- Zipcode: Shows the zipcode in address field.\n",
    "- Country: Shows the country in address field.\n",
    "- Timezone: Shows timezone based on the location of the accident (eastern, central, etc.).\n",
    "\n",
    "Weather Attributes (11):\n",
    "\n",
    "- Airport_Code: Denotes an airport-based weather station which is the closest one to location of the accident.\n",
    "- Weather_Timestamp: Shows the time-stamp of weather observation record (in local time).\n",
    "- Temperature(F): Shows the temperature (in Fahrenheit).\n",
    "- Wind_Chill(F): Shows the wind chill (in Fahrenheit).\n",
    "- Humidity(%): Shows the humidity (in percentage).\n",
    "- Pressure(in): Shows the air pressure (in inches).\n",
    "- Visibility(mi): Shows visibility (in miles).\n",
    "- Wind_Direction: Shows wind direction.\n",
    "- Wind_Speed(mph): Shows wind speed (in miles per hour).\n",
    "- Precipitation(in): Shows precipitation amount in inches, if there is any.\n",
    "- Weather_Condition: Shows the weather condition (rain, snow, thunderstorm, fog, etc.).\n",
    "\n",
    "POI Attributes (13):\n",
    "\n",
    "- Amenity: A Point-Of-Interest (POI) annotation which indicates presence of amenity in a nearby location.\n",
    "- Bump: A POI annotation which indicates presence of speed bump or hump in a nearby location.\n",
    "- Crossing: A POI annotation which indicates presence of crossing in a nearby location.\n",
    "- Give_Way: A POI annotation which indicates presence of give_way sign in a nearby location.\n",
    "- Junction: A POI annotation which indicates presence of junction in a nearby location.\n",
    "- No_Exit: A POI annotation which indicates presence of no_exit sign in a nearby location.\n",
    "- Railway: A POI annotation which indicates presence of railway in a nearby location.\n",
    "- Roundabout: A POI annotation which indicates presence of roundabout in a nearby location.\n",
    "- Station: A POI annotation which indicates presence of station (bus, train, etc.) in a nearby location.\n",
    "- Stop: A POI annotation which indicates presence of stop sign in a nearby location.\n",
    "- Traffic_Calming: A POI annotation which indicates presence of traffic_calming means in a nearby location.\n",
    "- Traffic_Signal: A POI annotation which indicates presence of traffic_signal in a nearby location.\n",
    "- Turning_Loop: A POI annotation which indicates presence of turning_loop in a nearby location.\n",
    "\n",
    "Period-of-Day (4):\n",
    "\n",
    "- Sunrise_Sunset: Shows the period of day (i.e. day or night) based on sunrise/sunset.\n",
    "- Civil_Twilight: Shows the period of day (i.e. day or night) based on civil twilight.\n",
    "- Nautical_Twilight: Shows the period of day (i.e. day or night) based on nautical twilight.\n",
    "- Astronomical_Twilight: Shows the period of day (i.e. day or night) based on astronomical twilight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89589382-89fd-41b9-9c73-319f82156ee5",
   "metadata": {},
   "source": [
    "### 2.2. Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54df775b-7017-49c1-bf9a-cfa5c727ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar para contagem\n",
    "spark_df = spark_df.withColumn(\"Count\", lit(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd5d49-1401-45ad-8de8-d07c1730ed03",
   "metadata": {},
   "source": [
    "#### 2.2.1. Analisando variável de resposta (gravidade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbc1a5-7105-4f1a-b9ce-9ad81e4acc82",
   "metadata": {},
   "source": [
    "Criando variável de duração do acidente (em minutos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "684646a4-a544-4eb7-8417-349435aa2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# arrumar variáveis de timestamp\n",
    "spark_df = spark_df.withColumn('Start_Time',to_timestamp('Start_Time'))\n",
    "spark_df = spark_df.withColumn('End_Time',to_timestamp('End_Time'))\n",
    "\n",
    "# calculando duração do acidente\n",
    "spark_df = spark_df.withColumn(\"Duration\",round(col('End_Time').cast(\"long\") - col('Start_Time').cast(\"long\"))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe73aed3-f0e5-403b-af4f-313d4efa524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A média de duração geral em minutos é"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "172db343-9ecb-4876-aca7-549cfc4e2d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 18:55:21 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "21/09/29 18:55:22 WARN TaskSetManager: Stage 0 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     avg(Duration)|\n",
      "+------------------+\n",
      "|280.52224770854576|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.agg(avg(col(\"Duration\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b878ab6-85d7-44a6-ae08-544a4b44b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "Analisando a gravidade em termos de duração do acidente e distância do trânsito afetada "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27f81c63-9130-43de-be93-27c466111701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 19:18:16 WARN TaskSetManager: Stage 69 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 69:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-------------------+\n",
      "|Severity|     avg(Duration)|  avg(Distance(mi))|\n",
      "+--------+------------------+-------------------+\n",
      "|       1| 40.28549991719377|0.20158595352385564|\n",
      "|       2|243.86602116055516| 0.5045440084341835|\n",
      "|       3| 219.4669417331046| 0.6062612072475774|\n",
      "|       4| 813.8796330921866| 1.5317024608780312|\n",
      "+--------+------------------+-------------------+\n",
      "\n",
      "CPU times: user 4.98 ms, sys: 19.7 ms, total: 24.6 ms\n",
      "Wall time: 3.33 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "group1 = spark_df.groupBy(\"Severity\").avg(\"Duration\",\"Distance(mi)\")\n",
    "group1.sort(\"Severity\", ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d8a86c-e00c-4e23-a9ea-d4020da1b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "Quantidade de observações por nível de gravidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7576290-abe5-4726-8bb0-21abe5bab7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 19:18:45 WARN TaskSetManager: Stage 71 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 71:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Severity|sum(Count)|\n",
      "+--------+----------+\n",
      "|       1|     28178|\n",
      "|       2|   1212382|\n",
      "|       3|    161052|\n",
      "|       4|    114452|\n",
      "+--------+----------+\n",
      "\n",
      "CPU times: user 23.2 ms, sys: 419 µs, total: 23.7 ms\n",
      "Wall time: 3.09 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "group2 = spark_df.groupBy(\"Severity\").sum(\"Count\")\n",
    "group2.sort(\"Severity\", ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ce741-b56f-4e6f-a325-b91b9edda99d",
   "metadata": {},
   "source": [
    "#### 2.2.2. Criando variável de resposta binária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "94370e9f-3cce-42e5-a5af-62ebe7461731",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn('Severity4', when((spark_df.Severity == 4), lit(1)) \\\n",
    "                    .otherwise(lit(0)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87440563-ea8d-4304-a396-b51463b6b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conferindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6364cd26-023a-4f0d-b8aa-0815003390c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 21:27:24 WARN TaskSetManager: Stage 312 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 312:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|Severity|sum(Severity4)|\n",
      "+--------+--------------+\n",
      "|       1|             0|\n",
      "|       2|             0|\n",
      "|       3|             0|\n",
      "|       4|        114452|\n",
      "+--------+--------------+\n",
      "\n",
      "CPU times: user 19.7 ms, sys: 3.73 ms, total: 23.4 ms\n",
      "Wall time: 3.12 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "group3 = spark_df.groupBy(\"Severity\").sum('Severity4')\n",
    "group3.sort(\"Severity\", ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53b55ea-afc4-45e0-8f35-3627488d691a",
   "metadata": {},
   "source": [
    "Vendo distribuição por ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28eca3f1-98c7-4d2f-894f-75e31bb8f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn('Year',year('Start_Time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19f5698a-48bf-4724-b40d-1d463f05ca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 19:27:42 WARN TaskSetManager: Stage 75 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 75:=====================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "|Year|sum(Severity4)|\n",
      "+----+--------------+\n",
      "|2016|         14185|\n",
      "|2017|         23751|\n",
      "|2018|         25212|\n",
      "|2019|         24378|\n",
      "|2020|         26926|\n",
      "+----+--------------+\n",
      "\n",
      "CPU times: user 17.1 ms, sys: 4.79 ms, total: 21.9 ms\n",
      "Wall time: 3.31 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "group4 = spark_df.groupBy(\"Year\").sum('Severity4')\n",
    "group4.sort(\"Year\", ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e5c08-6787-4ead-944f-e39a580e91e1",
   "metadata": {},
   "source": [
    "### 2.3. Seleção de atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f592f9e9-38e9-4afe-bf77-dac45d9bbb1b",
   "metadata": {},
   "source": [
    "#### 2.3.1. Filtrando variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca088c-b21e-451d-8e1b-2cea07ab3b1c",
   "metadata": {},
   "source": [
    "Para a tarefa de predição, primeiro precisamos filtrar as variáveis que não são conhecidas antes do acontecimento do acidente.\n",
    "Também vamos filtrar variáveis de endereço, por serem muito especificas e para simplificar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7f5c922f-a2cb-4f2a-9b67-7eac33cfc68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = spark_df.drop('ID','Severity','Distance(mi)','Description','End_Time','End_Lat', 'End_Lng','Duration','Year'\n",
    "                        'State','Number','Street','Side','City','County','Zipcode','Airport_Code','Weather_Timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0e821-eeb2-4705-875d-1548bfabf2da",
   "metadata": {},
   "source": [
    "#### 2.3.2. Variáveis categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f8910fcc-4dd1-4115-9bb6-71923f13b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_names = ['Country', 'Timezone', 'Weather_Condition','Amenity', 'Bump', 'Crossing', \n",
    "             'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', \n",
    "             'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', \n",
    "             'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6a47e0d3-6540-424c-bcbd-44dd1f5b3c6d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:02:14 WARN TaskSetManager: Stage 167 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 20:02:17 WARN TaskSetManager: Stage 170 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT State)|\n",
      "+---------------------+\n",
      "|                   49|\n",
      "+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:02:20 WARN TaskSetManager: Stage 173 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Country)|\n",
      "+-----------------------+\n",
      "|                      1|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:02:23 WARN TaskSetManager: Stage 176 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT Timezone)|\n",
      "+------------------------+\n",
      "|                       5|\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|count(DISTINCT Weather_Condition)|\n",
      "+---------------------------------+\n",
      "|                              117|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:02:26 WARN TaskSetManager: Stage 179 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 20:02:29 WARN TaskSetManager: Stage 182 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Amenity)|\n",
      "+-----------------------+\n",
      "|                      2|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:02:32 WARN TaskSetManager: Stage 185 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT Bump)|\n",
      "+--------------------+\n",
      "|                   2|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:02:35 WARN TaskSetManager: Stage 188 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT Crossing)|\n",
      "+------------------------+\n",
      "|                       2|\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:02:38 WARN TaskSetManager: Stage 191 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT Give_Way)|\n",
      "+------------------------+\n",
      "|                       2|\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:02:41 WARN TaskSetManager: Stage 194 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT Junction)|\n",
      "+------------------------+\n",
      "|                       2|\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:02:44 WARN TaskSetManager: Stage 197 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT No_Exit)|\n",
      "+-----------------------+\n",
      "|                      2|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:02:46 WARN TaskSetManager: Stage 200 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Railway)|\n",
      "+-----------------------+\n",
      "|                      2|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:02:49 WARN TaskSetManager: Stage 203 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|count(DISTINCT Roundabout)|\n",
      "+--------------------------+\n",
      "|                         2|\n",
      "+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:02:52 WARN TaskSetManager: Stage 206 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Station)|\n",
      "+-----------------------+\n",
      "|                      2|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:02:55 WARN TaskSetManager: Stage 209 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT Stop)|\n",
      "+--------------------+\n",
      "|                   2|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:02:58 WARN TaskSetManager: Stage 212 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|count(DISTINCT Traffic_Calming)|\n",
      "+-------------------------------+\n",
      "|                              2|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|count(DISTINCT Traffic_Signal)|\n",
      "+------------------------------+\n",
      "|                             2|\n",
      "+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:03:01 WARN TaskSetManager: Stage 215 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 20:03:04 WARN TaskSetManager: Stage 218 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|count(DISTINCT Turning_Loop)|\n",
      "+----------------------------+\n",
      "|                           1|\n",
      "+----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:03:07 WARN TaskSetManager: Stage 221 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|count(DISTINCT Sunrise_Sunset)|\n",
      "+------------------------------+\n",
      "|                             3|\n",
      "+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:03:10 WARN TaskSetManager: Stage 224 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|count(DISTINCT Civil_Twilight)|\n",
      "+------------------------------+\n",
      "|                             3|\n",
      "+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 20:03:13 WARN TaskSetManager: Stage 227 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|count(DISTINCT Nautical_Twilight)|\n",
      "+---------------------------------+\n",
      "|                                3|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 227:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|count(DISTINCT Astronomical_Twilight)|\n",
      "+-------------------------------------+\n",
      "|                                    3|\n",
      "+-------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in cat_names:\n",
    "    df_clean.select(countDistinct(i)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35323ccc-b2ff-4b30-864b-c4b5d839e558",
   "metadata": {},
   "source": [
    "Filtrando 'Country' e'Turning_Loop', uma vez que só tem uma classe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "94426f0a-27f8-4285-9007-931fc4676801",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop('Country','Turning_Loop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d0de4-471c-4ea8-9b6e-3214c42baa28",
   "metadata": {},
   "source": [
    "### 2.4. Valores missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae12391-3fb9-448b-ab31-36b11a76d5e8",
   "metadata": {},
   "source": [
    "#### 2.4.1. Analisando casos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "98fd1b43-47fb-4261-846c-b053cd5d3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista de variaveis sem ser timestamp e boleeano\n",
    "mis_names = ['Start_Lat', 'Start_Lng', 'State','Timezone','Temperature(F)','Wind_Chill(F)','Humidity(%)',\n",
    "             'Pressure(in)', 'Visibility(mi)', 'Wind_Direction', 'Wind_Speed(mph)', 'Precipitation(in)','Weather_Condition',\n",
    "             'Sunrise_Sunset','Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6ebc949b-e9fb-4e64-84b9-6504e44403d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 21:07:58 WARN TaskSetManager: Stage 282 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 282:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+--------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+--------------+--------------+-----------------+---------------------+\n",
      "|Start_Lat|Start_Lng|State|Timezone|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|\n",
      "+---------+---------+-----+--------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+--------------+--------------+-----------------+---------------------+\n",
      "|        0|        0|    0|    2302|         43033|       449316|      45509|       36274|         44211|         41858|         128862|           510549|            44007|            83|            83|               83|                   83|\n",
      "+---------+---------+-----+--------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+--------------+--------------+-----------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_clean.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in mis_names]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e0374-7452-4a03-b359-4c4aa45348ff",
   "metadata": {},
   "source": [
    "Variáveis com grande quantidade de valores missing: Wind_Chill(F), Wind_Speed(mph) e Precipitation(in). A variável de chuva é bem importante, então vamos substituir os valores missing. As outras duas variáveis vamos dropar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9499b4cf-f83e-47a8-bd2f-e4e5a95f0a49",
   "metadata": {},
   "source": [
    "#### 2.4.2. Dropando variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "443c7b05-0f66-47ee-a86f-118e74e9ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop('Wind_Chill(F)','Wind_Speed(mph)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e8949-9698-4a8f-9558-cc73ab513a88",
   "metadata": {},
   "source": [
    "#### 2.4.3. Substituindo valores missing na variável 'Precipitation(in)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0cf783ae-ce04-45b5-8fe0-cc8fc53238a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identificando casos em que precipitação é NA\n",
    "df_clean = df_clean.withColumn('Precipitation_NA', when(isnan(col('Precipitation(in)')), lit(1))\n",
    "                    .otherwise(lit(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "840eaa0f-b2b1-4e40-842b-898455d74655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 21:28:15 WARN TaskSetManager: Stage 314 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 314:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|Precipitation_NA|sum(Count)|\n",
      "+----------------+----------+\n",
      "|               1|    510549|\n",
      "|               0|   1005515|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#conferindo\n",
    "df_clean.groupBy('Precipitation_NA').sum('Count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a67e94d8-906c-48e6-b7e5-c0df2df9e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "33db2d05-05aa-4c95-94ff-ebe42c9423c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 21:28:37 WARN TaskSetManager: Stage 324 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculando a mediana e média de precipitação\n",
    "median = df_clean.filter(df_clean['Precipitation_NA'] == 0).approxQuantile('Precipitation(in)', [0.5], 0.25)\n",
    "median = median[0]\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "274e4418-19cf-4090-a442-819e2203f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Substituindo pela mediana (0)\n",
    "df_clean = df_clean.na.fill(value=median,subset=[\"Precipitation(in)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "45133617-5d30-4ac2-9c2d-9d85ebf0d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop('Precipitation_NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a7c022dd-8c51-47ff-b09d-632e9d47ac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 21:28:48 WARN TaskSetManager: Stage 326 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 326:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+--------+--------------+-----------+------------+--------------+--------------+-----------------+-----------------+--------------+--------------+-----------------+---------------------+\n",
      "|Start_Lat|Start_Lng|State|Timezone|Temperature(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Precipitation(in)|Weather_Condition|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|\n",
      "+---------+---------+-----+--------+--------------+-----------+------------+--------------+--------------+-----------------+-----------------+--------------+--------------+-----------------+---------------------+\n",
      "|        0|        0|    0|    2302|         43033|      45509|       36274|         44211|         41858|                0|            44007|            83|            83|               83|                   83|\n",
      "+---------+---------+-----+--------+--------------+-----------+------------+--------------+--------------+-----------------+-----------------+--------------+--------------+-----------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Conferindo\n",
    "#lista de variaveis sem ser timestamp e boleeano\n",
    "mis_names2 = ['Start_Lat', 'Start_Lng', 'State','Timezone','Temperature(F)','Humidity(%)',\n",
    "             'Pressure(in)', 'Visibility(mi)', 'Wind_Direction', 'Precipitation(in)','Weather_Condition',\n",
    "             'Sunrise_Sunset','Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
    "\n",
    "df_clean.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in mis_names2]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc353e-edab-41df-9a55-c2d353328a64",
   "metadata": {},
   "source": [
    "#### 2.4.4. Dropando NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "daa6cc2a-576f-4d1c-a339-3567f832cb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 21:29:00 WARN TaskSetManager: Stage 328 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 328:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1516064, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Quantidade de linhas antes\n",
    "print((df_clean.count(), len(df_clean.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "cde73f40-2277-40cb-87d7-0e340a99b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean2 = df_clean.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4c8145ed-d0c5-407d-ad71-79510fda528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 21:29:10 WARN TaskSetManager: Stage 330 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 330:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460889, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((df_clean2.count(), len(df_clean2.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3661c93-5ebb-4fd1-a5a3-7ebaec6460d6",
   "metadata": {},
   "source": [
    "### 2.5. Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0fd4842d-9c3e-415b-a08f-cc74a795fe2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 21:34:51 WARN TaskSetManager: Stage 350 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:34:54 WARN TaskSetManager: Stage 352 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:34:57 WARN TaskSetManager: Stage 354 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:35:00 WARN TaskSetManager: Stage 356 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq. classe majoritária: 1350815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 356:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq. classe minoritária: 110074\n",
      "ratio: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "major_df = df_clean2.filter(col(\"Severity4\") == 0)\n",
    "minor_df = df_clean2.filter(col(\"Severity4\") == 1)\n",
    "ratio = int(major_df.count()/minor_df.count())\n",
    "\n",
    "print(\"Freq. classe majoritária: {}\".format(major_df.count()))\n",
    "print(\"Freq. classe minoritária: {}\".format(minor_df.count()))\n",
    "print(\"ratio: {}\".format(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b01aa-0415-4291-acc7-08b91e821c53",
   "metadata": {},
   "source": [
    "A classe de gravidade 4 aparece numa proporção de 1:12 na base. Vamos fazer oversampling e undersampling pra ajustar essa desproporcionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ac1c4244-c4a2-4ed2-90bb-8cc6de12216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersampling\n",
    "sampled_majority_df = major_df.sample(False, 1/4, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "787c9910-2379-4cb4-838b-540ae6301483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversampling\n",
    "a = range(3)\n",
    "# duplicate the minority rows\n",
    "oversampled_minority_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "96e50641-3f5f-4523-985b-f029d4bddfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base final\n",
    "resampled_df = sampled_majority_df.unionAll(oversampled_minority_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56970dee-ed74-49f3-8e8a-abee466f40f8",
   "metadata": {},
   "source": [
    "Conferindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b208ab61-3df7-4e02-8ef3-c6a1f1e27f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 21:40:56 WARN TaskSetManager: Stage 370 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:41:02 WARN TaskSetManager: Stage 372 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:41:07 WARN TaskSetManager: Stage 374 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:41:13 WARN TaskSetManager: Stage 376 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq. classe majoritária: 337205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 376:============================>                           (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq. classe minoritária: 330222\n",
      "ratio: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "major_df2 = resampled_df.filter(col(\"Severity4\") == 0)\n",
    "minor_df2 = resampled_df.filter(col(\"Severity4\") == 1)\n",
    "ratio2 = int(major_df2.count()/minor_df2.count())\n",
    "\n",
    "print(\"Freq. classe majoritária: {}\".format(major_df2.count()))\n",
    "print(\"Freq. classe minoritária: {}\".format(minor_df2.count()))\n",
    "print(\"ratio: {}\".format(ratio2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221456d3-d6ce-486d-890f-21c026902550",
   "metadata": {},
   "source": [
    "### 2.6. Base final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "93dfecbe-1649-4e07-84a2-2ce7456d34d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 21:40:47 WARN TaskSetManager: Stage 368 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 368:=========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(667427, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((resampled_df.count(), len(resampled_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "462698c7-574e-47ef-a80e-baf5227cac9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Start_Time: timestamp (nullable = true)\n",
      " |-- Start_Lat: float (nullable = true)\n",
      " |-- Start_Lng: float (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Timezone: string (nullable = true)\n",
      " |-- Temperature(F): float (nullable = true)\n",
      " |-- Humidity(%): float (nullable = true)\n",
      " |-- Pressure(in): float (nullable = true)\n",
      " |-- Visibility(mi): float (nullable = true)\n",
      " |-- Wind_Direction: string (nullable = true)\n",
      " |-- Precipitation(in): float (nullable = false)\n",
      " |-- Weather_Condition: string (nullable = true)\n",
      " |-- Amenity: boolean (nullable = true)\n",
      " |-- Bump: boolean (nullable = true)\n",
      " |-- Crossing: boolean (nullable = true)\n",
      " |-- Give_Way: boolean (nullable = true)\n",
      " |-- Junction: boolean (nullable = true)\n",
      " |-- No_Exit: boolean (nullable = true)\n",
      " |-- Railway: boolean (nullable = true)\n",
      " |-- Roundabout: boolean (nullable = true)\n",
      " |-- Station: boolean (nullable = true)\n",
      " |-- Stop: boolean (nullable = true)\n",
      " |-- Traffic_Calming: boolean (nullable = true)\n",
      " |-- Traffic_Signal: boolean (nullable = true)\n",
      " |-- Sunrise_Sunset: string (nullable = true)\n",
      " |-- Civil_Twilight: string (nullable = true)\n",
      " |-- Nautical_Twilight: string (nullable = true)\n",
      " |-- Astronomical_Twilight: string (nullable = true)\n",
      " |-- Severity4: integer (nullable = false)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resampled_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506e342-9798-4c5e-bdd3-d28ef69f7169",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecc49cb-622e-419a-8826-86c3534f9d17",
   "metadata": {},
   "source": [
    "### 3.1 VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "aa979531-8ee6-4d6f-8ee9-4f5db9def5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|Severity4|\n",
      "+--------------------+---------+\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5,...|        0|\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5,...|        0|\n",
      "|(19,[0,1,2,3,4,5,...|        0|\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5,...|        0|\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5]...|        0|\n",
      "|(19,[0,1,2,3,4,5,...|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 21:47:29 WARN TaskSetManager: Stage 378 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numeric_cols = ['Start_Lat', 'Start_Lng', 'Temperature(F)','Humidity(%)',\n",
    "             'Pressure(in)', 'Visibility(mi)', 'Precipitation(in)',\n",
    "             'Amenity', 'Bump', 'Crossing','Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', \n",
    "            'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal']\n",
    "va = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "\n",
    "df = va.transform(resampled_df)\n",
    "df = df.select(['features', 'Severity4'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcb260a-48ee-47e9-ab51-dd410ef057c8",
   "metadata": {},
   "source": [
    "### 3.2 Separação em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b3253bb6-1497-4e3a-9c66-91c5cf222977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 21:48:16 WARN TaskSetManager: Stage 379 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:48:23 WARN TaskSetManager: Stage 381 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 500256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 381:================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Count: 167171\n",
      "CPU times: user 17 ms, sys: 8.21 ms, total: 25.2 ms\n",
      "Wall time: 13.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, test = df.randomSplit([0.75, 0.25], seed = 10)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46de22b-f53d-4c1d-8e71-3c10c2eb8ffb",
   "metadata": {},
   "source": [
    "### 3.3 Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c00d44a6-e565-4f53-985b-f9714cbc8254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 21:49:38 WARN TaskSetManager: Stage 383 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:49:45 WARN TaskSetManager: Stage 385 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:49:46 WARN TaskSetManager: Stage 386 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:49:52 WARN TaskSetManager: Stage 387 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:49:59 WARN TaskSetManager: Stage 389 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:50:06 WARN TaskSetManager: Stage 391 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:50:07 WARN TaskSetManager: Stage 393 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:50:08 WARN TaskSetManager: Stage 395 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/09/29 21:50:09 WARN TaskSetManager: Stage 397 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61 ms, sys: 16.5 ms, total: 77.5 ms\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'Severity4', seed = 10)\n",
    "model = rf.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b0b549-dadd-48a8-83fa-6df652011b47",
   "metadata": {},
   "source": [
    "### 3.4 Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "e47c84d6-4f1f-4ec2-8dc4-c8cb0be5bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ca6c16ce-c30e-43bd-8cfa-eb4392aa91a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 21:50:59 WARN TaskSetManager: Stage 399 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 399:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|Severity4|prediction|\n",
      "+---------+----------+\n",
      "|        0|       1.0|\n",
      "|        0|       1.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "|        0|       0.0|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#pegando so as colunas de interesse\n",
    "selected = predictions.select(['Severity4','prediction'])\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d0e11-a7cf-4e6c-9854-a9956295f354",
   "metadata": {},
   "source": [
    "### 3.5 Avaliando a acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "70e4062e-cf94-4a3d-a347-c72f22ef723a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 22:02:40 WARN TaskSetManager: Stage 413 contains a task of very large size (54032 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 413:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.1 ms, sys: 4.49 ms, total: 19.5 ms\n",
      "Wall time: 6.39 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.671300644250498"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#calculando a acurácia\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#criando objeto da classe\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Severity4', \\\n",
    "                                             predictionCol = 'prediction', \\\n",
    "                                             metricName='accuracy')\n",
    "\n",
    "accuracy = evaluator.evaluate(selected)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46196508-b34e-4166-8775-4bd6dcef823c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Rascunho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821451e-5683-498c-a8a7-70c583383843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e6a9d3c-1fa6-4c8c-8fb9-bda9144f2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando no spark\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5babdab-0b7b-41e8-b37d-9374c1bcf01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .appName(\"my_app\") \\\n",
    "            .config('spark.sql.codegen.wholeStage', False) \\\n",
    "            .getOrCreate()\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAQ7W6N2BJJAHR5AKF\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"eu-west-3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ffd130c-0173-4a90-9ced-d131a8a12ddf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o129.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:795)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 25 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15/1638930617.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://beatriz-yaginuma/trabalho-computacao-nuvem/US_Accidents_Dec20_updated.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o129.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:795)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"s3a://beatriz-yaginuma/trabalho-computacao-nuvem/US_Accidents_Dec20_updated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "281068c7-769d-4fd9-b050-2199be161a54",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=car_accidents, master=local) created by __init__ at /tmp/ipykernel_15/1799683984.py:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15/2855043084.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"car_accidents\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    343\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=car_accidents, master=local) created by __init__ at /tmp/ipykernel_15/1799683984.py:5 "
     ]
    }
   ],
   "source": [
    "# Criando sessão do Spark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\",\"car_accidents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3017ea0-7a46-4c90-9327-1225c26ba94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando um novo contexto SQL em cima do contexto existente\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ccd817-fe0f-4e52-8583-99d956b8c349",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16/4134826275.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24751dfd-8807-44f8-ba39-851b96b5178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Olhando apenas para um subconjunto dos dados\n",
    "pandas_df = pandas_df[['Severity','Start_Time','Start_Lat','Start_Lng','State','Country','Timezone',\n",
    "                       'Temperature(F)','Wind_Chill(F)','Humidity(%)','Pressure(in)','Visibility(mi)',\n",
    "                       'Wind_Direction','Wind_Speed(mph)','Precipitation(in)', 'Weather_Condition',\n",
    "                       'Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station',\n",
    "                       'Stop','Traffic_Calming','Traffic_Signal','Turning_Loop',\n",
    "                       'Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6a54776-9e25-4725-a7a4-675f38669adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Severity',\n",
       " 'Start_Time',\n",
       " 'Start_Lat',\n",
       " 'Start_Lng',\n",
       " 'Side',\n",
       " 'State',\n",
       " 'Country',\n",
       " 'Timezone',\n",
       " 'Temperature(F)',\n",
       " 'Wind_Chill(F)',\n",
       " 'Humidity(%)',\n",
       " 'Pressure(in)',\n",
       " 'Visibility(mi)',\n",
       " 'Wind_Direction',\n",
       " 'Wind_Speed(mph)',\n",
       " 'Precipitation(in)',\n",
       " 'Weather_Condition',\n",
       " 'Amenity',\n",
       " 'Bump',\n",
       " 'Crossing',\n",
       " 'Give_Way',\n",
       " 'Junction',\n",
       " 'No_Exit',\n",
       " 'Railway',\n",
       " 'Roundabout',\n",
       " 'Station',\n",
       " 'Stop',\n",
       " 'Traffic_Calming',\n",
       " 'Traffic_Signal',\n",
       " 'Turning_Loop',\n",
       " 'Sunrise_Sunset',\n",
       " 'Civil_Twilight',\n",
       " 'Nautical_Twilight',\n",
       " 'Astronomical_Twilight']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_pandas.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e6308f7-1926-46ce-af0e-0bf34d018e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dtype('int64'),\n",
       " dtype('O'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('O'),\n",
       " dtype('O'),\n",
       " dtype('O'),\n",
       " dtype('O'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('O'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('O'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('O'),\n",
       " dtype('O'),\n",
       " dtype('O'),\n",
       " dtype('O')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_pandas.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae681a1e-776e-47f1-9f23-3da5f1405087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definindo schema \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "mySchema = StructType([ StructField('Severity', StringType(), True)\\\n",
    "                       ,StructField(\"Quantidade vinculos ativos\", IntegerType(), True)\\\n",
    "                       ,StructField(\"UF\", IntegerType(), True)\\\n",
    "                       ,StructField(\"Ind Simples\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5589914f-da5a-4c68-aa3c-9e273c4a9370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Auxiliar functions\n",
    "def equivalent_type(f):\n",
    "    if f == 'datetime64[ns]': return TimestampType()\n",
    "    elif f == 'int64': return LongType()\n",
    "    elif f == 'int32': return IntegerType()\n",
    "    elif f == 'float64': return FloatType()\n",
    "    elif f == 'bool': return BooleanType()\n",
    "    else: return StringType()\n",
    "\n",
    "def define_structure(string, pandas_dtype):\n",
    "    try: spark_dtype = equivalent_type(pandas_dtype)\n",
    "    except: spark_dtype = StringType()\n",
    "    return StructField(string, spark_dtype)\n",
    "\n",
    "# Given pandas dataframe, it will return a spark's dataframe.\n",
    "def pandas_to_spark(pandas_df):\n",
    "    columns = list(pandas_df.columns)\n",
    "    types = list(pandas_df.dtypes)\n",
    "    struct_list = []\n",
    "    for column, typo in zip(columns, types): \n",
    "      struct_list.append(define_structure(column, typo))\n",
    "    p_schema = StructType(struct_list)\n",
    "    return spark.createDataFrame(pandas_df, p_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e1d74af-d34c-41c5-afb9-b554bb232770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- input: string (nullable = true)\n",
      " |-- output: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- input: timestamp (nullable = true)\n",
      " |-- output: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df=spark.createDataFrame(\n",
    "        data = [ (\"2013-02-03 22:50:57\",\"2013-02-03 22:55:57\")],\n",
    "        schema=[\"input\",\"output\"])\n",
    "df.printSchema()\n",
    "\n",
    "df = df.withColumn(\"input\",to_timestamp(\"input\"))\n",
    "df = df.withColumn(\"output\",to_timestamp(\"output\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44b4c46a-b893-4ff0-aeb3-2724b9d787e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------+\n",
      "|              input|             output|duration|\n",
      "+-------------------+-------------------+--------+\n",
      "|2013-02-03 22:50:57|2013-02-03 22:55:57|       0|\n",
      "+-------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"duration\", datediff(col(\"output\"),col(\"input\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "852eb6f0-c788-45df-a1d8-ed4ce387d474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------+\n",
      "|              input|             output|duration|\n",
      "+-------------------+-------------------+--------+\n",
      "|2013-02-03 22:50:57|2013-02-03 22:55:57|     5.0|\n",
      "+-------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"duration\",round(col(\"output\").cast(\"long\") - col('input').cast(\"long\"))/60).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806f8d6-5eca-4129-868d-7ea4f6d95d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.select(mean(col(\"duration\"))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78fd27bf-b59b-41b4-9841-a6e1a9a64f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 4]\r"
     ]
    }
   ],
   "source": [
    "spark_df = spark_df.withColumnRenamed(\"duration\",\"Duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb1d81e1-49ec-48a0-8af2-0fa8cd05ef69",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_163/387239269.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m sns.boxplot(x=\"Severity\", y=\"Duration\",\n\u001b[0m\u001b[1;32m      6\u001b[0m             data=spark_df, palette=\"Set2\", ax=axs[0])\n\u001b[1;32m      7\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accidents Duration by Severity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/seaborn/_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mboxplot\u001b[0;34m(x, y, hue, data, order, hue_order, orient, color, palette, saturation, width, dodge, fliersize, linewidth, whis, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2241\u001b[0m ):\n\u001b[1;32m   2242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2243\u001b[0;31m     plotter = _BoxPlotter(x, y, hue, data, order, hue_order,\n\u001b[0m\u001b[1;32m   2244\u001b[0m                           \u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2245\u001b[0m                           width, dodge, fliersize, linewidth)\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, orient, color, palette, saturation, width, dodge, fliersize, linewidth)\u001b[0m\n\u001b[1;32m    404\u001b[0m                  width, dodge, fliersize, linewidth):\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestablish_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mestablish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# See if we need to get variables from `data`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mhue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \"\"\"\n\u001b[1;32m   1642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1644\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'get'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "sns.boxplot(x=\"Severity\", y=\"Duration\",\n",
    "            data=spark_df, palette=\"Set2\", ax=axs[0])\n",
    "fig.suptitle('Accidents Duration by Severity', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ee5a5-05e6-40ab-8151-465de8600f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bc1c43e-46f5-4f4a-b612-b93d4921c577",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_163/594965685.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m sns.boxplot(x=\"Severity\", y=\"Duration\",\n\u001b[0m\u001b[1;32m      5\u001b[0m             data=spark_df, palette=\"Set2\")\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/seaborn/_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mboxplot\u001b[0;34m(x, y, hue, data, order, hue_order, orient, color, palette, saturation, width, dodge, fliersize, linewidth, whis, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2241\u001b[0m ):\n\u001b[1;32m   2242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2243\u001b[0;31m     plotter = _BoxPlotter(x, y, hue, data, order, hue_order,\n\u001b[0m\u001b[1;32m   2244\u001b[0m                           \u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2245\u001b[0m                           width, dodge, fliersize, linewidth)\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, orient, color, palette, saturation, width, dodge, fliersize, linewidth)\u001b[0m\n\u001b[1;32m    404\u001b[0m                  width, dodge, fliersize, linewidth):\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestablish_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mestablish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# See if we need to get variables from `data`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mhue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \"\"\"\n\u001b[1;32m   1642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1644\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(x=\"Severity\", y=\"Duration\",\n",
    "            data=spark_df, palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b777de37-1b40-4f57-855b-03cadc21bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Pressure_bc']= boxcox(df['Pressure(in)'].apply(lambda x: x+1),lmbda=6)\n",
    "df['Visibility_bc']= boxcox(df['Visibility(mi)'].apply(lambda x: x+1),lmbda = 0.1)\n",
    "df['Wind_Speed_bc']= boxcox(df['Wind_Speed(mph)'].apply(lambda x: x+1),lmbda=-0.2)\n",
    "df = df.drop(['Pressure(in)','Visibility(mi)','Wind_Speed(mph)'], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
