{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f98ba25-8f47-48ed-aa35-172595a07424",
   "metadata": {},
   "source": [
    "## 0. Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904bf8a-4d96-4b52-98c6-ac1e7f40c582",
   "metadata": {},
   "source": [
    "## 1. Importando dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd53e22b-cc2f-4135-a8b2-a92a88256683",
   "metadata": {},
   "source": [
    "### 1.1. Importando dados do S3 para pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9453e18f-3f88-449c-a2b6-fba31085317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# criando sessão do s3\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id = 'AKIAQ7W6N2BJJAHR5AKF',\n",
    "    aws_secret_access_key='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'\n",
    ")\n",
    "\n",
    "#criando objeto s3, que vai criar a conexão\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "#criando o arquivo que vamos importar do s3\n",
    "s3object = s3.Object('beatriz-yaginuma','trabalho-computacao-nuvem/US_Accidents_Dec20_updated.csv')\n",
    "s3object.get()\n",
    "\n",
    "#fazendo chamada e jogando dados num objeto\n",
    "file = s3object.get()['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c18590-046e-4eb3-9acd-f137ba7d14b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.02 s, sys: 958 ms, total: 8.98 s\n",
      "Wall time: 9.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "#importando no pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcac6364-9805-4d6f-bd83-4b8b26e10495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1516064, 47)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253993f-07bd-42d6-af03-9299fed0f529",
   "metadata": {},
   "source": [
    "### 1.2. Importando do Pandas para Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d031d6b-1837-4dd5-a6f3-78f4a97327b7",
   "metadata": {},
   "source": [
    "Ao invés de criar um contexto para cada interação específica do Spark, vamos criar uma sessão, que é uma combinação de todos os contextos diferentes do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94c2755-462b-46b3-9193-852721be34ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 05:08:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('trabalho') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49a4ef-d1d9-42a5-a9df-e2800df1ec08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#criando dataframe\n",
    "spark_df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f534e-c243-4f2d-8069-2f7c814ae3c2",
   "metadata": {},
   "source": [
    "Problema: algumas colunas na base tem valores ausentes, o que leva o Pandas a representá-los como tipos mistos (string para não ausentes, NaN para valores ausentes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352094b-3c7a-4f2b-9752-f4ebbc6e2cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(pandas_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388f0c9-35d4-4034-b304-c56ed9277709",
   "metadata": {},
   "source": [
    "Para resolver isso, precisamos fornecer um schema para a função createDataFrame.\n",
    "Vamos usar algumas funções para automatizar a construção do schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e6543a-640b-4adf-a938-15ea5ac794ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Auxiliar functions\n",
    "def equivalent_type(f):\n",
    "    if f == 'datetime64[ns]': return TimestampType()\n",
    "    elif f == 'int64': return LongType()\n",
    "    elif f == 'int32': return IntegerType()\n",
    "    elif f == 'float64': return FloatType()\n",
    "    elif f == 'bool': return BooleanType()\n",
    "    else: return StringType()\n",
    "\n",
    "def define_structure(string, pandas_dtype):\n",
    "    try: spark_dtype = equivalent_type(pandas_dtype)\n",
    "    except: spark_dtype = StringType()\n",
    "    return StructField(string, spark_dtype)\n",
    "\n",
    "# Given pandas dataframe, it will return a spark's dataframe.\n",
    "def define_schema(pandas_df):\n",
    "    columns = list(pandas_df.columns)\n",
    "    types = list(pandas_df.dtypes)\n",
    "    struct_list = []\n",
    "    for column, typo in zip(columns, types): \n",
    "      struct_list.append(define_structure(column, typo))\n",
    "    return StructType(struct_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984853aa-edf7-4f1d-aba9-bf30e70eeab8",
   "metadata": {},
   "source": [
    "- A primeira função tem como input o tipo de dados do pandas, e retorna o tipo equivalente do Spark.\n",
    "- A segunda função tem como input o nome de uma variável e seu tipo de dados do pandas, e aplica a primeira função para retornar o StructField da variável + o tipo de dados do Spark\n",
    "- A terceira função tem como input o dataframe do pandas, e constro listas de todas as columas e seus respectivos tipos do dataframe para aplicar a segunda função e criar uma terceira lista de todos os struct types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66407dfc-7c90-490e-9722-bed3fcb923d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(ID,StringType,true),StructField(Severity,LongType,true),StructField(Start_Time,StringType,true),StructField(End_Time,StringType,true),StructField(Start_Lat,FloatType,true),StructField(Start_Lng,FloatType,true),StructField(End_Lat,FloatType,true),StructField(End_Lng,FloatType,true),StructField(Distance(mi),FloatType,true),StructField(Description,StringType,true),StructField(Number,FloatType,true),StructField(Street,StringType,true),StructField(Side,StringType,true),StructField(City,StringType,true),StructField(County,StringType,true),StructField(State,StringType,true),StructField(Zipcode,StringType,true),StructField(Country,StringType,true),StructField(Timezone,StringType,true),StructField(Airport_Code,StringType,true),StructField(Weather_Timestamp,StringType,true),StructField(Temperature(F),FloatType,true),StructField(Wind_Chill(F),FloatType,true),StructField(Humidity(%),FloatType,true),StructField(Pressure(in),FloatType,true),StructField(Visibility(mi),FloatType,true),StructField(Wind_Direction,StringType,true),StructField(Wind_Speed(mph),FloatType,true),StructField(Precipitation(in),FloatType,true),StructField(Weather_Condition,StringType,true),StructField(Amenity,BooleanType,true),StructField(Bump,BooleanType,true),StructField(Crossing,BooleanType,true),StructField(Give_Way,BooleanType,true),StructField(Junction,BooleanType,true),StructField(No_Exit,BooleanType,true),StructField(Railway,BooleanType,true),StructField(Roundabout,BooleanType,true),StructField(Station,BooleanType,true),StructField(Stop,BooleanType,true),StructField(Traffic_Calming,BooleanType,true),StructField(Traffic_Signal,BooleanType,true),StructField(Turning_Loop,BooleanType,true),StructField(Sunrise_Sunset,StringType,true),StructField(Civil_Twilight,StringType,true),StructField(Nautical_Twilight,StringType,true),StructField(Astronomical_Twilight,StringType,true)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a função\n",
    "p_schema = define_schema(pandas_df)\n",
    "p_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d74b6-8115-4772-9fd6-bcc18784a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Importando os dados para o Spark, agora definindo o Schema\n",
    "spark_df = spark.createDataFrame(pandas_df, p_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3e314-c90c-47aa-bc9e-a4d2c757b04d",
   "metadata": {},
   "source": [
    "## 2. Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b2d23-2b52-4f23-b454-ca191fc8b3ad",
   "metadata": {},
   "source": [
    "### 2.1. Descrição da base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a55facb9-01aa-431b-a515-e513d5abff33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Severity: long (nullable = true)\n",
      " |-- Start_Time: string (nullable = true)\n",
      " |-- Start_Lat: float (nullable = true)\n",
      " |-- Start_Lng: float (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Timezone: string (nullable = true)\n",
      " |-- Temperature(F): float (nullable = true)\n",
      " |-- Wind_Chill(F): float (nullable = true)\n",
      " |-- Humidity(%): float (nullable = true)\n",
      " |-- Pressure(in): float (nullable = true)\n",
      " |-- Visibility(mi): float (nullable = true)\n",
      " |-- Wind_Direction: string (nullable = true)\n",
      " |-- Wind_Speed(mph): float (nullable = true)\n",
      " |-- Precipitation(in): float (nullable = true)\n",
      " |-- Weather_Condition: string (nullable = true)\n",
      " |-- Amenity: boolean (nullable = true)\n",
      " |-- Bump: boolean (nullable = true)\n",
      " |-- Crossing: boolean (nullable = true)\n",
      " |-- Give_Way: boolean (nullable = true)\n",
      " |-- Junction: boolean (nullable = true)\n",
      " |-- No_Exit: boolean (nullable = true)\n",
      " |-- Railway: boolean (nullable = true)\n",
      " |-- Roundabout: boolean (nullable = true)\n",
      " |-- Station: boolean (nullable = true)\n",
      " |-- Stop: boolean (nullable = true)\n",
      " |-- Traffic_Calming: boolean (nullable = true)\n",
      " |-- Traffic_Signal: boolean (nullable = true)\n",
      " |-- Turning_Loop: boolean (nullable = true)\n",
      " |-- Sunrise_Sunset: string (nullable = true)\n",
      " |-- Civil_Twilight: string (nullable = true)\n",
      " |-- Nautical_Twilight: string (nullable = true)\n",
      " |-- Astronomical_Twilight: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analisando os dados\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470bd6b0-cdc1-4ee1-be08-65b75f78f449",
   "metadata": {},
   "source": [
    "Traffic Attributes (4):\n",
    "\n",
    "- ID: This is a unique identifier of the accident record.\n",
    "Source: Indicates source of the accident report (i.e. the API which reported the accident.).\n",
    "- TMC: A traffic accident may have a Traffic Message Channel (TMC) code which provides more detailed description of the event.\n",
    "- Severity: Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on traffic (i.e., short delay as a result of the accident) and 4 indicates a significant impact on traffic (i.e., long delay).\n",
    "- Start_Time: Shows start time of the accident in local time zone.\n",
    "- End_Time: Shows end time of the accident in local time zone.\n",
    "- Start_Lat: Shows latitude in GPS coordinate of the start point.\n",
    "- Start_Lng: Shows longitude in GPS coordinate of the start point.\n",
    "- End_Lat: Shows latitude in GPS coordinate of the end point.\n",
    "- End_Lng: Shows longitude in GPS coordinate of the end point.\n",
    "- Distance(mi): The length of the road extent affected by the accident.\n",
    "- Description: Shows natural language description of the accident.\n",
    "\n",
    "Address Attributes (3):\n",
    "\n",
    "- State: Shows the state in address field.\n",
    "- Country: Shows the country in address field.\n",
    "- Timezone: Shows timezone based on the location of the accident (eastern, central, etc.).\n",
    "\n",
    "Weather Attributes (9):\n",
    "\n",
    "- Temperature(F): Shows the temperature (in Fahrenheit).\n",
    "- Wind_Chill(F): Shows the wind chill (in Fahrenheit).\n",
    "- Humidity(%): Shows the humidity (in percentage).\n",
    "- Pressure(in): Shows the air pressure (in inches).\n",
    "- Visibility(mi): Shows visibility (in miles).\n",
    "- Wind_Direction: Shows wind direction.\n",
    "- Wind_Speed(mph): Shows wind speed (in miles per hour).\n",
    "- Precipitation(in): Shows precipitation amount in inches, if there is any.\n",
    "- Weather_Condition: Shows the weather condition (rain, snow, thunderstorm, fog, etc.).\n",
    "\n",
    "POI Attributes (13):\n",
    "\n",
    "- Amenity: A Point-Of-Interest (POI) annotation which indicates presence of amenity in a nearby location.\n",
    "- Bump: A POI annotation which indicates presence of speed bump or hump in a nearby location.\n",
    "- Crossing: A POI annotation which indicates presence of crossing in a nearby location.\n",
    "- Give_Way: A POI annotation which indicates presence of give_way sign in a nearby location.\n",
    "- Junction: A POI annotation which indicates presence of junction in a nearby location.\n",
    "- No_Exit: A POI annotation which indicates presence of no_exit sign in a nearby location.\n",
    "- Railway: A POI annotation which indicates presence of railway in a nearby location.\n",
    "- Roundabout: A POI annotation which indicates presence of roundabout in a nearby location.\n",
    "- Station: A POI annotation which indicates presence of station (bus, train, etc.) in a nearby location.\n",
    "- Stop: A POI annotation which indicates presence of stop sign in a nearby location.\n",
    "- Traffic_Calming: A POI annotation which indicates presence of traffic_calming means in a nearby location.\n",
    "- Traffic_Signal: A POI annotation which indicates presence of traffic_signal in a nearby location.\n",
    "- Turning_Loop: A POI annotation which indicates presence of turning_loop in a nearby location.\n",
    "\n",
    "Period-of-Day (4):\n",
    "\n",
    "- Sunrise_Sunset: Shows the period of day (i.e. day or night) based on sunrise/sunset.\n",
    "- Civil_Twilight: Shows the period of day (i.e. day or night) based on civil twilight.\n",
    "- Nautical_Twilight: Shows the period of day (i.e. day or night) based on nautical twilight.\n",
    "- Astronomical_Twilight: Shows the period of day (i.e. day or night) based on astronomical twilight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89589382-89fd-41b9-9c73-319f82156ee5",
   "metadata": {},
   "source": [
    "### 2.2. Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46196508-b34e-4166-8775-4bd6dcef823c",
   "metadata": {},
   "source": [
    "## Rascunho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821451e-5683-498c-a8a7-70c583383843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e6a9d3c-1fa6-4c8c-8fb9-bda9144f2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando no spark\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5babdab-0b7b-41e8-b37d-9374c1bcf01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .appName(\"my_app\") \\\n",
    "            .config('spark.sql.codegen.wholeStage', False) \\\n",
    "            .getOrCreate()\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAQ7W6N2BJJAHR5AKF\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"eu-west-3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ffd130c-0173-4a90-9ced-d131a8a12ddf",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o129.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:795)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 25 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15/1638930617.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://beatriz-yaginuma/trabalho-computacao-nuvem/US_Accidents_Dec20_updated.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o129.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:795)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"s3a://beatriz-yaginuma/trabalho-computacao-nuvem/US_Accidents_Dec20_updated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "281068c7-769d-4fd9-b050-2199be161a54",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=car_accidents, master=local) created by __init__ at /tmp/ipykernel_15/1799683984.py:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15/2855043084.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"car_accidents\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    343\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=car_accidents, master=local) created by __init__ at /tmp/ipykernel_15/1799683984.py:5 "
     ]
    }
   ],
   "source": [
    "# Criando sessão do Spark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\",\"car_accidents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3017ea0-7a46-4c90-9327-1225c26ba94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando um novo contexto SQL em cima do contexto existente\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ccd817-fe0f-4e52-8583-99d956b8c349",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16/4134826275.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24751dfd-8807-44f8-ba39-851b96b5178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Olhando apenas para um subconjunto dos dados\n",
    "pandas_df = pandas_df[['Severity','Start_Time','Start_Lat','Start_Lng','State','Country','Timezone',\n",
    "                       'Temperature(F)','Wind_Chill(F)','Humidity(%)','Pressure(in)','Visibility(mi)',\n",
    "                       'Wind_Direction','Wind_Speed(mph)','Precipitation(in)', 'Weather_Condition',\n",
    "                       'Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station',\n",
    "                       'Stop','Traffic_Calming','Traffic_Signal','Turning_Loop',\n",
    "                       'Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6a54776-9e25-4725-a7a4-675f38669adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Severity',\n",
       " 'Start_Time',\n",
       " 'Start_Lat',\n",
       " 'Start_Lng',\n",
       " 'Side',\n",
       " 'State',\n",
       " 'Country',\n",
       " 'Timezone',\n",
       " 'Temperature(F)',\n",
       " 'Wind_Chill(F)',\n",
       " 'Humidity(%)',\n",
       " 'Pressure(in)',\n",
       " 'Visibility(mi)',\n",
       " 'Wind_Direction',\n",
       " 'Wind_Speed(mph)',\n",
       " 'Precipitation(in)',\n",
       " 'Weather_Condition',\n",
       " 'Amenity',\n",
       " 'Bump',\n",
       " 'Crossing',\n",
       " 'Give_Way',\n",
       " 'Junction',\n",
       " 'No_Exit',\n",
       " 'Railway',\n",
       " 'Roundabout',\n",
       " 'Station',\n",
       " 'Stop',\n",
       " 'Traffic_Calming',\n",
       " 'Traffic_Signal',\n",
       " 'Turning_Loop',\n",
       " 'Sunrise_Sunset',\n",
       " 'Civil_Twilight',\n",
       " 'Nautical_Twilight',\n",
       " 'Astronomical_Twilight']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_pandas.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e6308f7-1926-46ce-af0e-0bf34d018e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dtype('int64'),\n",
       " dtype('O'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('O'),\n",
       " dtype('O'),\n",
       " dtype('O'),\n",
       " dtype('O'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('O'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('O'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('bool'),\n",
       " dtype('O'),\n",
       " dtype('O'),\n",
       " dtype('O'),\n",
       " dtype('O')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_pandas.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae681a1e-776e-47f1-9f23-3da5f1405087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definindo schema \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "mySchema = StructType([ StructField('Severity', StringType(), True)\\\n",
    "                       ,StructField(\"Quantidade vinculos ativos\", IntegerType(), True)\\\n",
    "                       ,StructField(\"UF\", IntegerType(), True)\\\n",
    "                       ,StructField(\"Ind Simples\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5589914f-da5a-4c68-aa3c-9e273c4a9370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Auxiliar functions\n",
    "def equivalent_type(f):\n",
    "    if f == 'datetime64[ns]': return TimestampType()\n",
    "    elif f == 'int64': return LongType()\n",
    "    elif f == 'int32': return IntegerType()\n",
    "    elif f == 'float64': return FloatType()\n",
    "    elif f == 'bool': return BooleanType()\n",
    "    else: return StringType()\n",
    "\n",
    "def define_structure(string, pandas_dtype):\n",
    "    try: spark_dtype = equivalent_type(pandas_dtype)\n",
    "    except: spark_dtype = StringType()\n",
    "    return StructField(string, spark_dtype)\n",
    "\n",
    "# Given pandas dataframe, it will return a spark's dataframe.\n",
    "def pandas_to_spark(pandas_df):\n",
    "    columns = list(pandas_df.columns)\n",
    "    types = list(pandas_df.dtypes)\n",
    "    struct_list = []\n",
    "    for column, typo in zip(columns, types): \n",
    "      struct_list.append(define_structure(column, typo))\n",
    "    p_schema = StructType(struct_list)\n",
    "    return spark.createDataFrame(pandas_df, p_schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
